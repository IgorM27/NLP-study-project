{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer, Self-Attention и моделирование языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:04.225361Z",
     "start_time": "2019-11-05T18:27:04.223470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle,\n",
    "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
    "\n",
    "# !git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
    "# import sys; sys.path.append('./stepik-dl-nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:53.780539Z",
     "start_time": "2019-11-05T18:27:52.444607Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import heapq\n",
    "import random\n",
    "import copy\n",
    "import datetime\n",
    "import random\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "import collections\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
    "\n",
    "\n",
    "def tokenize_text_simple_regex(txt, min_token_size=4):\n",
    "    txt = txt.lower()\n",
    "    all_tokens = TOKEN_RE.findall(txt)\n",
    "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
    "\n",
    "\n",
    "def character_tokenize(txt):\n",
    "    return list(txt)\n",
    "\n",
    "\n",
    "def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
    "    return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
    "\n",
    "\n",
    "def add_fake_token(word2id, token='<PAD>'):\n",
    "    word2id_new = {token: i + 1 for token, i in word2id.items()}\n",
    "    word2id_new[token] = 0\n",
    "    return word2id_new\n",
    "\n",
    "\n",
    "def texts_to_token_ids(tokenized_texts, word2id):\n",
    "    return [[word2id[token] for token in text if token in word2id]\n",
    "            for text in tokenized_texts]\n",
    "\n",
    "\n",
    "def build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None):\n",
    "    word_counts = collections.defaultdict(int)\n",
    "    doc_n = 0\n",
    "\n",
    "    # посчитать количество документов, в которых употребляется каждое слово\n",
    "    # а также общее количество документов\n",
    "    for txt in tokenized_texts:\n",
    "        doc_n += 1\n",
    "        unique_text_tokens = set(txt)\n",
    "        for token in unique_text_tokens:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "    # убрать слишком редкие и слишком частые слова\n",
    "    word_counts = {word: cnt for word, cnt in word_counts.items()\n",
    "                   if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n",
    "\n",
    "    # отсортировать слова по убыванию частоты\n",
    "    sorted_word_counts = sorted(word_counts.items(),\n",
    "                                reverse=True,\n",
    "                                key=lambda pair: pair[1])\n",
    "\n",
    "    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n",
    "    if pad_word is not None:\n",
    "        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n",
    "\n",
    "    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n",
    "    if len(word_counts) > max_size:\n",
    "        sorted_word_counts = sorted_word_counts[:max_size]\n",
    "\n",
    "    # нумеруем слова\n",
    "    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n",
    "\n",
    "    # нормируем частоты слов\n",
    "    word2freq = np.array([cnt / doc_n for _, cnt in sorted_word_counts], dtype='float32')\n",
    "\n",
    "    return word2id, word2freq\n",
    "\n",
    "\n",
    "PAD_TOKEN = '__PAD__'\n",
    "NUMERIC_TOKEN = '__NUMBER__'\n",
    "NUMERIC_RE = re.compile(r'^([0-9.,e+\\-]+|[mcxvi]+)$', re.I)\n",
    "\n",
    "\n",
    "def load_war_and_piece_chunks(fname, chunk_size=200):\n",
    "    with open(fname, 'r', encoding='utf-8', errors='ignore') as fin:\n",
    "        full_text = fin.read()\n",
    "    return [full_text[start:start + chunk_size] for start in range(0, len(full_text), chunk_size // 2)]\n",
    "\n",
    "def ensure_length(txt, out_len, pad_value):\n",
    "    if len(txt) < out_len:\n",
    "        txt = list(txt) + [pad_value] * (out_len - len(txt))\n",
    "    else:\n",
    "        txt = txt[:out_len]\n",
    "    return txt\n",
    "\n",
    "def save_texts_to_file(texts, out_file):\n",
    "    with open(out_file, 'w') as outf:\n",
    "        outf.write('\\n'.join(texts))\n",
    "\n",
    "def replace_number_nokens(tokenized_texts):\n",
    "    return [[token if not NUMERIC_RE.match(token) else NUMERIC_TOKEN for token in text]\n",
    "            for text in tokenized_texts]\n",
    "\n",
    "class LanguageModelDataset(Dataset):\n",
    "    def __init__(self, token_ids, chunk_length=100, pad_value=0):\n",
    "        self.token_ids = token_ids\n",
    "        self.chunk_length = chunk_length\n",
    "        self.pad_value = pad_value\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_ids)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.token_ids[item]\n",
    "        start_i = random.randint(0, max(0, len(text) - self.chunk_length - 1))\n",
    "        chunk = text[start_i : start_i + self.chunk_length + 1]\n",
    "\n",
    "        seed_part = chunk[:-1]\n",
    "        target_part = chunk[1:]\n",
    "\n",
    "        seed_part = ensure_length(seed_part, self.chunk_length, self.pad_value)\n",
    "        target_part = ensure_length(target_part, self.chunk_length, self.pad_value)\n",
    "\n",
    "        seed_part = np.array(seed_part)\n",
    "        target_part = np.array(target_part)\n",
    "\n",
    "        return seed_part, target_part\n",
    "\n",
    "\n",
    "class GreedyGenerator:\n",
    "    def __init__(self, model, tokenizer, device='cuda', eos_token_id=3):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(self.device)\n",
    "        self.eos_token_id = eos_token_id\n",
    "\n",
    "    def __call__(self, seed_text, max_steps_n=40):\n",
    "        seed_tokens = self.tokenizer.encode([seed_text])[0]\n",
    "\n",
    "        for _ in range(max_steps_n):\n",
    "            in_batch = torch.tensor(seed_tokens).unsqueeze(0).to(self.device)\n",
    "            best_next_token = self.model(in_batch)[0, -1].argmax()\n",
    "            if best_next_token == self.eos_token_id:\n",
    "                break\n",
    "\n",
    "            seed_tokens.append(best_next_token)\n",
    "\n",
    "        return self.tokenizer.decode([seed_tokens])[0]\n",
    "\n",
    "\n",
    "class BeamGenerator:\n",
    "    def __init__(self, model, tokenizer, device='cuda', eos_token_id=3):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(self.device)\n",
    "        self.eos_token_id = eos_token_id\n",
    "\n",
    "    def __call__(self, seed_text, max_steps_n=40, return_hypotheses_n=5, beamsize=5):\n",
    "        seed_tokens = self.tokenizer.encode([seed_text])[0]\n",
    "        initial_length = len(seed_tokens)\n",
    "\n",
    "        partial_hypotheses = [(0, seed_tokens)]\n",
    "        final_hypotheses = []\n",
    "\n",
    "        while len(partial_hypotheses) > 0:\n",
    "            cur_partial_score, cur_partial_hypothesis = heapq.heappop(partial_hypotheses)\n",
    "\n",
    "            in_batch = torch.tensor(cur_partial_hypothesis).unsqueeze(0).to(self.device)\n",
    "            next_tokens_logits = self.model(in_batch)[0, -1]\n",
    "            next_tokens_logproba = F.log_softmax(next_tokens_logits)\n",
    "            topk_continuations = next_tokens_logproba.topk(beamsize)\n",
    "\n",
    "            for token_score, token_idx in zip(topk_continuations.values, topk_continuations.indices):\n",
    "                token_score = float(token_score)\n",
    "                token_idx = int(token_idx)\n",
    "\n",
    "                old_denorm_score = cur_partial_score * np.sqrt(len(cur_partial_hypothesis))\n",
    "                new_score = (old_denorm_score - token_score) / np.sqrt(len(cur_partial_hypothesis) + 1)\n",
    "\n",
    "                new_hypothesis = cur_partial_hypothesis + [token_idx]\n",
    "                new_item = (new_score, new_hypothesis)\n",
    "\n",
    "                if token_idx == self.eos_token_id or len(new_hypothesis) - initial_length >= max_steps_n:\n",
    "                    final_hypotheses.append(new_item)\n",
    "                else:\n",
    "                    heapq.heappush(partial_hypotheses, new_item)\n",
    "\n",
    "            if len(partial_hypotheses) > beamsize:\n",
    "                partial_hypotheses = heapq.nsmallest(beamsize, partial_hypotheses)\n",
    "                heapq.heapify(partial_hypotheses)\n",
    "\n",
    "        final_scores, final_token_lists = zip(*final_hypotheses)\n",
    "        final_texts = self.tokenizer.decode(list(final_token_lists))\n",
    "\n",
    "        result = list(zip(final_scores, final_texts))\n",
    "        result.sort()\n",
    "        result = result[:return_hypotheses_n]\n",
    "\n",
    "        return result\n",
    "\n",
    "def init_random_seed(value=0):\n",
    "    random.seed(value)\n",
    "    np.random.seed(value)\n",
    "    torch.manual_seed(value)\n",
    "    torch.cuda.manual_seed(value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def copy_data_to_device(data, device):\n",
    "    if torch.is_tensor(data):\n",
    "        return data.to(device)\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        return [copy_data_to_device(elem, device) for elem in data]\n",
    "    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n",
    "\n",
    "def train_eval_loop(model, train_dataset, val_dataset, criterion,\n",
    "                    lr=1e-4, epoch_n=10, batch_size=32,\n",
    "                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n",
    "                    max_batches_per_epoch_train=10000,\n",
    "                    max_batches_per_epoch_val=1000,\n",
    "                    data_loader_ctor=DataLoader,\n",
    "                    optimizer_ctor=None,\n",
    "                    lr_scheduler_ctor=None,\n",
    "                    shuffle_train=True,\n",
    "                    dataloader_workers_n=0):\n",
    "    \"\"\"\n",
    "    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n",
    "    :param model: torch.nn.Module - обучаемая модель\n",
    "    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n",
    "    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n",
    "    :param criterion: функция потерь для настройки модели\n",
    "    :param lr: скорость обучения\n",
    "    :param epoch_n: максимальное количество эпох\n",
    "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
    "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
    "    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n",
    "        отсутствие улучшения модели, чтобы обучение продолжалось.\n",
    "    :param l2_reg_alpha: коэффициент L2-регуляризации\n",
    "    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n",
    "    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n",
    "    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n",
    "        (по умолчанию torch.utils.data.DataLoader)\n",
    "    :return: кортеж из двух элементов:\n",
    "        - среднее значение функции потерь на валидации на лучшей эпохе\n",
    "        - лучшая модель\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device)\n",
    "    model.to(device)\n",
    "\n",
    "    if optimizer_ctor is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
    "    else:\n",
    "        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n",
    "\n",
    "    if lr_scheduler_ctor is not None:\n",
    "        lr_scheduler = lr_scheduler_ctor(optimizer)\n",
    "    else:\n",
    "        lr_scheduler = None\n",
    "\n",
    "    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n",
    "                                        num_workers=dataloader_workers_n)\n",
    "    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                                      num_workers=dataloader_workers_n)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch_i = 0\n",
    "    best_model = copy.deepcopy(model)\n",
    "\n",
    "    for epoch_i in range(epoch_n):\n",
    "        try:\n",
    "            epoch_start = datetime.datetime.now()\n",
    "            print('Эпоха {}'.format(epoch_i))\n",
    "\n",
    "            model.train()\n",
    "            mean_train_loss = 0\n",
    "            train_batches_n = 0\n",
    "            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
    "                if batch_i > max_batches_per_epoch_train:\n",
    "                    break\n",
    "\n",
    "                batch_x = copy_data_to_device(batch_x, device)\n",
    "                batch_y = copy_data_to_device(batch_y, device)\n",
    "\n",
    "                pred = model(batch_x)\n",
    "                loss = criterion(pred, batch_y)\n",
    "\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                mean_train_loss += float(loss)\n",
    "                train_batches_n += 1\n",
    "\n",
    "            mean_train_loss /= train_batches_n\n",
    "            print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n",
    "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n",
    "            print('Среднее значение функции потерь на обучении', mean_train_loss)\n",
    "\n",
    "\n",
    "\n",
    "            model.eval()\n",
    "            mean_val_loss = 0\n",
    "            val_batches_n = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n",
    "                    if batch_i > max_batches_per_epoch_val:\n",
    "                        break\n",
    "\n",
    "                    batch_x = copy_data_to_device(batch_x, device)\n",
    "                    batch_y = copy_data_to_device(batch_y, device)\n",
    "\n",
    "                    pred = model(batch_x)\n",
    "                    loss = criterion(pred, batch_y)\n",
    "\n",
    "                    mean_val_loss += float(loss)\n",
    "                    val_batches_n += 1\n",
    "\n",
    "            mean_val_loss /= val_batches_n\n",
    "            print('Среднее значение функции потерь на валидации', mean_val_loss)\n",
    "\n",
    "            if mean_val_loss < best_val_loss:\n",
    "                best_epoch_i = epoch_i\n",
    "                best_val_loss = mean_val_loss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print('Новая лучшая модель!')\n",
    "            elif epoch_i - best_epoch_i > early_stopping_patience:\n",
    "                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n",
    "                    early_stopping_patience))\n",
    "                break\n",
    "\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.step(mean_val_loss)\n",
    "\n",
    "            print()\n",
    "        except KeyboardInterrupt:\n",
    "            print('Досрочно остановлено пользователем')\n",
    "            break\n",
    "        except Exception as ex:\n",
    "            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n",
    "            break\n",
    "\n",
    "    return best_val_loss, best_model\n",
    "\n",
    "def get_params_number(model):\n",
    "    return sum(t.numel() for t in model.parameters())\n",
    "\n",
    "init_random_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка текстов и разбиение на обучающую и тестовую подвыборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:55.233798Z",
     "start_time": "2019-11-05T18:27:55.197616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7976"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "all_chunks = load_war_and_piece_chunks('datasets/war_and_peace.txt')\n",
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:55.402080Z",
     "start_time": "2019-11-05T18:27:55.379575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "у нее был грипп, как она говорила (грипп был тогда новое\n",
      "слово, употреблявшееся только редкими). В записочках, разосланных утром с\n",
      "красным лакеем, было написано без различия во всех:\n",
      "  \"Si vous n'avez\n"
     ]
    }
   ],
   "source": [
    "print(all_chunks[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:55.954154Z",
     "start_time": "2019-11-05T18:27:55.919185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки 5583\n",
      "Размер валидационной выборки 2393\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(all_chunks)\n",
    "\n",
    "TRAIN_SPLIT = int(len(all_chunks) * 0.7)\n",
    "train_texts = all_chunks[:TRAIN_SPLIT]\n",
    "test_texts = all_chunks[TRAIN_SPLIT:]\n",
    "\n",
    "print('Размер обучающей выборки', len(train_texts))\n",
    "print('Размер валидационной выборки', len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация корпуса с помощью BPE\n",
    "\n",
    "BPE - Byte Pair Encoding\n",
    "\n",
    "YouTokenToMe - быстрая реализация BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:56.412237Z",
     "start_time": "2019-11-05T18:27:56.386089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "BPE_MODEL_FILENAME = './models/war_and_peace_bpe.yttm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:56.928780Z",
     "start_time": "2019-11-05T18:27:56.696400Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_texts_to_file_my(texts, out_file):\n",
    "    with open(out_file, 'w', encoding=\"utf-8\") as outf:\n",
    "        outf.write('\\n'.join(texts))\n",
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "TRAIN_TEXTS_FILENAME = './datasets/war_and_peace_bpe_train.txt'\n",
    "save_texts_to_file_my(train_texts, TRAIN_TEXTS_FILENAME)\n",
    "yttm.BPE.train(data=TRAIN_TEXTS_FILENAME, vocab_size=1000, model=BPE_MODEL_FILENAME);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:57.767294Z",
     "start_time": "2019-11-05T18:27:57.731252Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = yttm.BPE(BPE_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:57.897826Z",
     "start_time": "2019-11-05T18:27:57.874631Z"
    }
   },
   "outputs": [],
   "source": [
    "print(' '.join(tokenizer.vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:58.100551Z",
     "start_time": "2019-11-05T18:27:58.075268Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(tokenizer.encode(train_texts[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:59.729717Z",
     "start_time": "2019-11-05T18:27:59.551045Z"
    }
   },
   "outputs": [],
   "source": [
    "train_token_ids = tokenizer.encode(train_texts, bos=True, eos=True)\n",
    "test_token_ids = tokenizer.encode(test_texts, bos=True, eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:00.401753Z",
     "start_time": "2019-11-05T18:27:59.731680Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist([len(sent) for sent in train_token_ids], bins=30)\n",
    "plt.title('Распределение длин фрагментов в токенах')\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:01.153867Z",
     "start_time": "2019-11-05T18:28:00.404320Z"
    }
   },
   "outputs": [],
   "source": [
    "token_counts = np.bincount([token_id for text in train_token_ids for token_id in text])\n",
    "plt.hist(token_counts, bins=100)\n",
    "plt.title('Распределение количества упоминаний токенов')\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:01.204527Z",
     "start_time": "2019-11-05T18:28:01.156884Z"
    }
   },
   "outputs": [],
   "source": [
    "unknown_subwords_in_test = sum(1 for text in test_token_ids for token_id in text if token_id == 1)\n",
    "print('Количество случаев с неизвестными n-граммами символов в валидационной выборке',\n",
    "      unknown_subwords_in_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка датасетов для PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:02.980335Z",
     "start_time": "2019-11-05T18:28:02.938616Z"
    }
   },
   "outputs": [],
   "source": [
    "CHUNK_LENGTH = 80\n",
    "\n",
    "train_dataset = LanguageModelDataset(train_token_ids,\n",
    "                                     chunk_length=CHUNK_LENGTH)\n",
    "test_dataset = LanguageModelDataset(test_token_ids,\n",
    "                                    chunk_length=CHUNK_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:03.085890Z",
     "start_time": "2019-11-05T18:28:03.061652Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:03.260915Z",
     "start_time": "2019-11-05T18:28:03.219571Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(list(train_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общие классы и функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Маска зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:04.302365Z",
     "start_time": "2019-11-05T18:28:04.244547Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_target_dependency_mask(length):\n",
    "    full_mask = torch.ones(length, length)\n",
    "    ignore_mask = torch.tril(full_mask) < 1\n",
    "    full_mask.masked_fill_(ignore_mask, float('-inf'))\n",
    "    full_mask.masked_fill_(~ignore_mask, 0)\n",
    "    return full_mask\n",
    "\n",
    "make_target_dependency_mask(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кодирование позиции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:05.590059Z",
     "start_time": "2019-11-05T18:28:05.567602Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_positional_encoding(max_length, embedding_size):\n",
    "    time = np.pi * torch.arange(0, max_length).float()\n",
    "    freq_dividers = torch.arange(1, embedding_size // 2 + 1).float()\n",
    "    inputs = time[:, None] / freq_dividers[None, :]\n",
    "    \n",
    "    result = torch.zeros(max_length, embedding_size)\n",
    "    result[:, 0::2] = torch.sin(inputs)\n",
    "    result[:, 1::2] = torch.cos(inputs)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:06.060293Z",
     "start_time": "2019-11-05T18:28:05.708626Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_pos_codes = make_positional_encoding(30, 30)\n",
    "plt.plot(sample_pos_codes[:, ::3].numpy());\n",
    "plt.gcf().set_size_inches((15, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основной класс - языковая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:07.079279Z",
     "start_time": "2019-11-05T18:28:07.031056Z"
    }
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, backbone, emb_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        self.emb_dropout = nn.Dropout(emb_dropout)\n",
    "        self.backbone = backbone\n",
    "        self.out = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "    def forward(self, seed_token_ids):\n",
    "        \"\"\"\n",
    "            seed_token_ids - BatchSize x MaxInLen\n",
    "        \"\"\"\n",
    "        batch_size, max_in_length = seed_token_ids.shape\n",
    "\n",
    "        seed_padding_mask = seed_token_ids == 0\n",
    "        dependency_mask = make_target_dependency_mask(max_in_length) \\\n",
    "            .to(seed_token_ids.device)\n",
    "        \n",
    "        seed_embs = self.embeddings(seed_token_ids)  # BatchSize x MaxInLen x EmbSize\n",
    "        pos_codes = make_positional_encoding(max_in_length,\n",
    "                                             self.embedding_size).unsqueeze(0).to(seed_embs.device)\n",
    "        seed_embs = seed_embs + pos_codes\n",
    "        seed_embs = self.emb_dropout(seed_embs)\n",
    "\n",
    "        # BatchSize x TargetLen x EmbSize\n",
    "        target_features = seed_embs\n",
    "        target_features = self.backbone(seed_embs,\n",
    "                                        mask=dependency_mask,\n",
    "                                        src_key_padding_mask=seed_padding_mask)\n",
    "        logits = self.out(target_features)  # BatchSize x TargetLen x VocabSize\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Утилиты для обучения - функция потерь и расписание изменения длины градиентного шага"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:07.797230Z",
     "start_time": "2019-11-05T18:28:07.774142Z"
    }
   },
   "outputs": [],
   "source": [
    "def lm_cross_entropy(pred, target):\n",
    "    \"\"\"\n",
    "    pred - BatchSize x TargetLen x VocabSize\n",
    "    target - BatchSize x TargetLen\n",
    "    \"\"\"\n",
    "    pred_flat = pred.view(-1, pred.shape[-1])  # BatchSize*TargetLen x VocabSize\n",
    "    target_flat = target.view(-1)  # BatchSize*TargetLen\n",
    "    return F.cross_entropy(pred_flat, target_flat, ignore_index=0)\n",
    "\n",
    "\n",
    "def lr_scheduler(optimizer):\n",
    "    return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                      patience=20,\n",
    "                                                      factor=0.5,\n",
    "                                                      verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация Transformer из PyTorch 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:09.401017Z",
     "start_time": "2019-11-05T18:28:09.365637Z"
    }
   },
   "outputs": [],
   "source": [
    "class BatchFirstTransformerEncoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.impl = nn.TransformerEncoder(*args, **kwargs)\n",
    "        self.initialize_weights()\n",
    "    \n",
    "    def forward(self, src, *args, **kwargs):\n",
    "        src = src.transpose(0, 1).contiguous()  # MaxInLen  x BatchSize x EmbSize\n",
    "        result = self.impl(src, *args, **kwargs)  # TargetLen x BatchSize x EmbSize\n",
    "        result = result.transpose(0, 1).contiguous()  # BatchSize x TargetLen x EmbSize\n",
    "        return result\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for param in self.impl.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:10.550078Z",
     "start_time": "2019-11-05T18:28:10.425261Z"
    }
   },
   "outputs": [],
   "source": [
    "torch_transf_model = LanguageModel(tokenizer.vocab_size(),\n",
    "                                   256,\n",
    "                                   BatchFirstTransformerEncoder(\n",
    "                                       nn.TransformerEncoderLayer(\n",
    "                                           d_model=256,\n",
    "                                           nhead=16,\n",
    "                                           dim_feedforward=512,\n",
    "                                           dropout=0.1),\n",
    "                                       num_layers=3),\n",
    "                                   emb_dropout=0.1)\n",
    "print('Количество параметров', get_params_number(torch_transf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:28:58.797642Z",
     "start_time": "2019-11-05T18:28:34.626744Z"
    }
   },
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_torch_transf_model) = train_eval_loop(torch_transf_model,\n",
    "                                            train_dataset,\n",
    "                                            test_dataset,\n",
    "                                            lm_cross_entropy,\n",
    "                                            lr=2e-3,\n",
    "                                            epoch_n=2000,\n",
    "                                            batch_size=512,\n",
    "                                            device='cuda',\n",
    "                                            early_stopping_patience=50,\n",
    "                                            max_batches_per_epoch_train=1000,\n",
    "                                            max_batches_per_epoch_val=1000,\n",
    "                                            lr_scheduler_ctor=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:00.752077Z",
     "start_time": "2019-11-05T18:29:00.675069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_torch_transf_model.state_dict(), './models/war_and_peace_torch_transf_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:00.844033Z",
     "start_time": "2019-11-05T18:29:00.789023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch_transf_model.load_state_dict(torch.load('./models/war_and_peace_torch_transf_best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация текста с помощью языковой модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Жадная генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:02.366423Z",
     "start_time": "2019-11-05T18:29:02.329495Z"
    }
   },
   "outputs": [],
   "source": [
    "greedy_generator = GreedyGenerator(torch_transf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:03.175509Z",
     "start_time": "2019-11-05T18:29:02.921960Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(greedy_generator('сказала княжна, оглядывая Бона'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:03.497702Z",
     "start_time": "2019-11-05T18:29:03.177598Z"
    }
   },
   "outputs": [],
   "source": [
    "print(greedy_generator('смеялась княжна, оглядывая Наполе'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:03.770265Z",
     "start_time": "2019-11-05T18:29:03.500330Z"
    }
   },
   "outputs": [],
   "source": [
    "print(greedy_generator('сказала княжна, оглядывая Кутуз'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:04.150676Z",
     "start_time": "2019-11-05T18:29:03.773669Z"
    }
   },
   "outputs": [],
   "source": [
    "print(greedy_generator('сказал Кутузов, оглядывая Наполеона'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация с помощью лучевого поиска - Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:08.328662Z",
     "start_time": "2019-11-05T18:29:08.294006Z"
    }
   },
   "outputs": [],
   "source": [
    "beam_generator = BeamGenerator(torch_transf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:10.573399Z",
     "start_time": "2019-11-05T18:29:09.653198Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "beam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n",
    "                                   beamsize=5,\n",
    "                                   return_hypotheses_n=5)\n",
    "\n",
    "for score, pred_txt in beam_gen_variants:\n",
    "    print('****')\n",
    "    print(score)\n",
    "    print(pred_txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:05.050342Z",
     "start_time": "2019-11-05T18:27:05.005Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "beam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n",
    "                                   beamsize=20,\n",
    "                                   return_hypotheses_n=20)\n",
    "\n",
    "for score, pred_txt in beam_gen_variants:\n",
    "    print('****')\n",
    "    print(score)\n",
    "    print(pred_txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:27:05.051378Z",
     "start_time": "2019-11-05T18:27:05.008Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "beam_gen_variants = beam_generator('сказала княжна, оглядывая Наполе',\n",
    "                                   beamsize=100,\n",
    "                                   return_hypotheses_n=20)\n",
    "\n",
    "for score, pred_txt in beam_gen_variants:\n",
    "    print('****')\n",
    "    print(score)\n",
    "    print(pred_txt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собственная реализация MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:13.586871Z",
     "start_time": "2019-11-05T18:29:13.544216Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_multihead_attention(queries, keys, values,\n",
    "                           keys_padding_mask, dependency_mask,\n",
    "                           is_training,\n",
    "                           weights_dropout):\n",
    "    \"\"\"\n",
    "    queries - BatchSize x ValuesLen x HeadN x KeySize\n",
    "    keys - BatchSize x KeysLen x HeadN x KeySize\n",
    "    values - BatchSize x KeysLen x HeadN x ValueSize\n",
    "    keys_padding_mask - BatchSize x KeysLen\n",
    "    dependency_mask - ValuesLen x KeysLen\n",
    "    is_training - bool\n",
    "    weights_dropout - float\n",
    "    \n",
    "    result - tuple of two:\n",
    "        - BatchSize x ValuesLen x HeadN x ValueSize - resulting features\n",
    "        - BatchSize x ValuesLen x KeysLen x HeadN - attention map\n",
    "    \"\"\"\n",
    "\n",
    "    # BatchSize x ValuesLen x KeysLen x HeadN\n",
    "    relevances = torch.einsum('bvhs,bkhs->bvkh', (queries, keys))\n",
    "    \n",
    "    # замаскировать элементы, выходящие за длины последовательностей ключей\n",
    "    padding_mask_expanded = keys_padding_mask[:, None, :, None].expand_as(relevances)\n",
    "    relevances.masked_fill_(padding_mask_expanded, float('-inf'))\n",
    "    \n",
    "    # замаскировать пары <выходная позиция, входная позиция>\n",
    "    relevances = relevances + dependency_mask[None, :, :, None].expand_as(relevances)\n",
    "    \n",
    "    normed_rels = F.softmax(relevances, dim=2)    \n",
    "    normed_rels = F.dropout(normed_rels, weights_dropout, is_training)\n",
    "    \n",
    "    # BatchSize x ValuesLen x KeysLen x HeadN x 1\n",
    "    normed_rels_expanded = normed_rels.unsqueeze(-1)\n",
    "    \n",
    "    # BatchSize x 1 x KeysLen x HeadN x ValueSize\n",
    "    values_expanded = values.unsqueeze(1)\n",
    "    \n",
    "    # BatchSize x ValuesLen x KeysLen x HeadN x ValueSize\n",
    "    weighted_values = normed_rels_expanded * values_expanded\n",
    "    result = weighted_values.sum(2)  # BatchSize x ValuesLen x HeadN x ValueSize\n",
    "    \n",
    "    return result, normed_rels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention - это Attention, в котором ключи, значения и запросы вычисляются из элементов одной и той же последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:14.090216Z",
     "start_time": "2019-11-05T18:29:14.064361Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyMultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, model_size, n_heads, dropout=0):\n",
    "        super().__init__()\n",
    "        assert model_size % n_heads == 0, 'Размерность модели должна делиться нацело на количество голов'\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.queries_proj = nn.Linear(model_size, model_size)\n",
    "        self.keys_proj = nn.Linear(model_size, model_size)\n",
    "        self.values_proj = nn.Linear(model_size, model_size)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.last_attention_map = None\n",
    "    \n",
    "    def forward(self, sequence, padding_mask, dependency_mask):\n",
    "        \"\"\"\n",
    "        sequence - BatchSize x Len x ModelSize\n",
    "        padding_mask - BatchSize x Len\n",
    "        dependency_mask - Len x Len\n",
    "        \n",
    "        result - BatchSize x Len x ModelSize\n",
    "        \"\"\"\n",
    "        batch_size, max_len, model_size = sequence.shape\n",
    "        \n",
    "        queries_flat = self.queries_proj(sequence)  # BatchSize x Len x ModelSize\n",
    "        queries = queries_flat.view(batch_size, max_len, self.n_heads, -1)\n",
    "        \n",
    "        keys_flat = self.keys_proj(sequence)  # BatchSize x Len x ModelSize\n",
    "        keys = keys_flat.view(batch_size, max_len, self.n_heads, -1)\n",
    "        \n",
    "        values_flat = self.values_proj(sequence)  # BatchSize x Len x ModelSize\n",
    "        values = values_flat.view(batch_size, max_len, self.n_heads, -1)\n",
    "        \n",
    "        # BatchSize x Len x HeadsN x ValueSize\n",
    "        result, att_map = my_multihead_attention(queries, keys, values,\n",
    "                                                 padding_mask, dependency_mask,\n",
    "                                                 self.training, self.dropout)\n",
    "        result_flat = result.view(batch_size, max_len, model_size)\n",
    "        \n",
    "        self.last_attention_map = att_map.detach()\n",
    "\n",
    "        return result_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Один слой трансформера - Self-Attention, Feed-Forward, skip-connections, LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:15.069352Z",
     "start_time": "2019-11-05T18:29:15.028453Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyTransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, model_size, n_heads, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attention = MyMultiheadSelfAttention(model_size,\n",
    "                                                       n_heads,\n",
    "                                                       dropout=dropout)\n",
    "        self.first_dropout = nn.Dropout(dropout)\n",
    "        self.first_norm = nn.LayerNorm(model_size)\n",
    "        \n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(model_size, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, model_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.second_norm = nn.LayerNorm(model_size)\n",
    "    \n",
    "    def forward(self, sequence, padding_mask, dependency_mask):\n",
    "        att_features = self.self_attention(sequence, padding_mask, dependency_mask)\n",
    "\n",
    "        sequence = sequence + self.first_dropout(att_features)\n",
    "        sequence = self.first_norm(sequence)\n",
    "        \n",
    "        sequence = sequence + self.feedforward(sequence)\n",
    "        sequence = self.second_norm(sequence)\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Энкодер Трансформера - стопка из нескольких слоёв"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:15.728916Z",
     "start_time": "2019-11-05T18:29:15.680640Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyTransformerEncoder(nn.Module):\n",
    "    def __init__(self, n_layers, **layer_kwargs):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            MyTransformerEncoderLayer(**layer_kwargs)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, sequence, mask, src_key_padding_mask):\n",
    "        for layer in self.layers:\n",
    "            sequence = layer(sequence, src_key_padding_mask, mask)\n",
    "        return sequence\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем обучить языковую модель с нашим Трансформером"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:18.531896Z",
     "start_time": "2019-11-05T18:29:18.460196Z"
    }
   },
   "outputs": [],
   "source": [
    "my_transf_model = LanguageModel(tokenizer.vocab_size(),\n",
    "                                256,\n",
    "                                MyTransformerEncoder(\n",
    "                                    n_layers=3,\n",
    "                                    model_size=256,\n",
    "                                    n_heads=16,\n",
    "                                    dim_feedforward=512,\n",
    "                                    dropout=0.1),\n",
    "                                emb_dropout=0.1)\n",
    "print('Количество параметров', get_params_number(my_transf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:30.000482Z",
     "start_time": "2019-11-05T18:29:24.291741Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(best_val_loss,\n",
    " best_my_transf_model) = train_eval_loop(my_transf_model,\n",
    "                                         train_dataset,\n",
    "                                         test_dataset,\n",
    "                                         lm_cross_entropy,\n",
    "                                         lr=2e-3,\n",
    "                                         epoch_n=2000,\n",
    "                                         batch_size=512,\n",
    "                                         device='cuda',\n",
    "                                         early_stopping_patience=50,\n",
    "                                         max_batches_per_epoch_train=1000,\n",
    "                                         max_batches_per_epoch_val=1000,\n",
    "                                         lr_scheduler_ctor=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:30.732005Z",
     "start_time": "2019-11-05T18:29:30.686738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "torch.save(best_my_transf_model.state_dict(), './models/war_and_peace_my_transf_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:30.853057Z",
     "start_time": "2019-11-05T18:29:30.811442Z"
    }
   },
   "outputs": [],
   "source": [
    "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
    "my_transf_model.load_state_dict(torch.load('./models/war_and_peace_my_transf_best.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Наша реализация - жадная генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:31.862429Z",
     "start_time": "2019-11-05T18:29:31.841831Z"
    }
   },
   "outputs": [],
   "source": [
    "my_greedy_generator = GreedyGenerator(my_transf_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:32.263263Z",
     "start_time": "2019-11-05T18:29:31.988891Z"
    }
   },
   "outputs": [],
   "source": [
    "my_greedy_generator('сказала княжна, оглядывая Андре')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализация карт внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:33.644923Z",
     "start_time": "2019-11-05T18:29:33.614615Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_attention_maps(model, input_string, tokenizer, device='cuda', max_heads=2, figsize=(16, 10)):\n",
    "    device = torch.device(device)\n",
    "\n",
    "    token_ids = tokenizer.encode([input_string])[0]\n",
    "\n",
    "    token_strs = [tokenizer.id_to_subword(i) for i in token_ids]\n",
    "    in_len = len(token_ids)\n",
    "    ticks = np.arange(0, in_len)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    in_batch = torch.tensor(token_ids).unsqueeze(0).to(device)\n",
    "    model(in_batch)\n",
    "\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, MyMultiheadSelfAttention):\n",
    "            cur_last_attention_map = module.last_attention_map[0].cpu().numpy()\n",
    "            n_heads = cur_last_attention_map.shape[-1]\n",
    "            n_heads_to_vis = min(n_heads, max_heads)\n",
    "\n",
    "            fig, axes = plt.subplots(1, n_heads_to_vis)\n",
    "            fig.set_size_inches(figsize)\n",
    "            for head_i in range(n_heads_to_vis):\n",
    "                ax = axes[head_i]\n",
    "                ax.imshow(cur_last_attention_map[..., head_i])\n",
    "\n",
    "                ax.set_yticks(ticks)\n",
    "                ax.set_ylim(bottom=in_len - 0.5, top=-0.5)\n",
    "                ax.set_yticklabels(token_strs)\n",
    "\n",
    "                ax.set_xticks(ticks)\n",
    "                ax.set_xticklabels(token_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-05T18:29:34.935250Z",
     "start_time": "2019-11-05T18:29:33.745970Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_attention_maps(my_transf_model, 'сказал Кутузов, оглядывая Бонапарта', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
